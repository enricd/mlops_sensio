{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# $ pip install fastapi[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py (basic app)\n",
    "\n",
    "from fastapi import FastAPI \n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Hello World\"}\n",
    "\n",
    "# run with: $ uvicorn app:app --reload\n",
    "# go to http://localhost:8000/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# app.py (app to predict digit on an image)\n",
    "\n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "from PIL import Image\n",
    "import onnxruntime as ort \n",
    "import numpy as np\n",
    "import io\n",
    "import math\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "ort_session = ort.InferenceSession('models/binary_classifier_3.onnx')\n",
    "TRHESHOLD = 0.5\n",
    "\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Hello World\"}\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(file: UploadFile = File(...)):\n",
    "    request_object_content = await file.read()\n",
    "    img = Image.open(io.BytesIO(request_object_content))\n",
    "    input = np.expand_dims(np.array(img, dtype=np.uint8), axis=0)\n",
    "    ort_inputs = {\n",
    "        \"input\": input\n",
    "    }\n",
    "    ort_output = ort_session.run(['output'], ort_inputs)[0]\n",
    "    output = sigmoid(ort_output)\n",
    "    return {\n",
    "        \"proba\": output,\n",
    "        \"label\": \"3\" if output > TRHESHOLD else \"no 3\"\n",
    "    }\n",
    "\n",
    "# Check the automatic generated docs of the API, where you can see every endopoint, parameters\n",
    "# description and even the possibility of testing it:\n",
    "# http://localhost:8000/docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install docker\n",
    "\n",
    "# Create a Dockerfile with:\n",
    "\n",
    "\"\"\"\n",
    "FROM continuumio/miniconda3\n",
    "\n",
    "RUN  conda install y -c conda-forge \\\n",
    "pillow \\\n",
    "    onnxruntime \\\n",
    "    fastapi \\\n",
    "    uvicorn \\\n",
    "    python-multipart \n",
    "\n",
    "COPY ./models /models\n",
    "COPY ./app.py /app.py\n",
    "\n",
    "CMD uvicorn app:app --host=0.0.0.0 --port=$PORT\n",
    "\"\"\"\n",
    "\n",
    "# Test it on your local env:\n",
    "# $ docker build -t dlops .\n",
    "# $ docker run -p 8000:8000 -e PORT=8000 dlops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Heroku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the Heroku CLI, then:\n",
    "\n",
    "# $ heroku login\n",
    "# $ heroku container:login\n",
    "# $ heroku container:push web -a <heroku-app-name>\n",
    "# $ heroku container:release web -a <heroku-app-name>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "446f2c25f8d3890a0c454a658a941dc96acd7ff126c35d9040e0238d696a4771"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
